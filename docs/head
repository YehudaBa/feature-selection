<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Student 1 Reichman University ID: 12345678" />
  <meta name="author" content="Student 2 Reichman University ID: 87654321" />
  <meta name="dcterms.date" content="2025-07-02" />
  <title> Final Project Report Project Name </title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title"> <strong>Final Project Report</strong><br />
<strong>Project Name</strong> </h1>
<p class="author"><strong>Student 1</strong><br />
Reichman University<br />
<code>ID: 12345678</code></p>
<p class="author"><strong>Student 2</strong><br />
Reichman University<br />
<code>ID: 87654321</code></p>
<p class="date">2025-07-02</p>
</header>
<h1 id="introduction">Introduction</h1>
<p>In today’s data-driven landscape, machine learning models are
frequently tasked with analyzing high-dimensional datasets—those
containing hundreds, thousands, or even millions of features. This
challenge is particularly pronounced in domains such as genomics,
cybersecurity, image and speech recognition, and general data science,
where data often includes a vast number of variables relative to the
number of observations <span class="citation"
data-cites="guyon2003introduction"></span>.</p>
<p>While having access to a large number of features might seem
advantageous, in practice, it often introduces serious complications.
Redundant, irrelevant, or noisy features can obscure the underlying
patterns in the data, leading to decreased predictive performance.
Additionally, they significantly increase the computational burden
during model training and inference, which is particularly problematic
when working under constraints of time or limited computing resources
<span class="citation" data-cites="datacamp2024curse"></span>. In
extreme cases, the presence of such features can even mislead the model,
causing it to learn spurious correlations instead of meaningful
insights.</p>
<p>Feature selection—a critical preprocessing step in the machine
learning pipeline—aims to address these challenges. By identifying and
retaining only the most relevant and informative features, feature
selection can improve model accuracy, reduce overfitting, accelerate
training, and enhance interpretability <span class="citation"
data-cites="kohavi1997wrappers"></span>. However, no single feature
selection technique works best in all scenarios. Some methods are
statistically rigorous but computationally intensive, making them
impractical for large-scale datasets. Others are fast and scalable but
may compromise on selection quality, potentially overlooking subtle but
important variables.</p>
<p>This project introduces a smart ensemble approach to feature
selection, which combines the strengths of multiple existing methods to
achieve a robust balance between performance (in terms of selection
quality and resulting model accuracy) and efficiency (in terms of
runtime and resource usage). Ensemble feature selection has been shown
to improve stability and accuracy by aggregating results from multiple
base methods <span class="citation"
data-cites="garcia2020big"></span>.</p>
<p>A key innovation of this approach lies in its iterative pipeline
structure. Instead of performing feature selection in a single pass, the
system proceeds through multiple controlled iterations. In each
iteration, it evaluates the feature set using the best available
model—determined dynamically based on current constraints and
performance metrics. These constraints, or regulations, include
user-defined parameters such as maximum runtime, memory limits, or a cap
on the number of features. The model choice at each step is adaptive:
for example, the system might choose a simple and fast model early in
the process to rapidly eliminate uninformative features, and then refine
the selection using a more complex and accurate model as the feature
space becomes smaller and more manageable.</p>
<p>This regulated, adaptive, and iterative process ensures that the
pipeline remains both efficient and effective across a wide range of use
cases and datasets. It continuously optimizes feature selection quality
while respecting practical constraints, striking an optimal trade-off
between speed and accuracy.</p>
<p>Furthermore, the implementation offers a high degree of user
customization and control. Users can specify a range of parameters
according to their needs and constraints, including:</p>
<ul>
<li><p>The target number of features (either as an exact number or a
percentage of the original set).</p></li>
<li><p>Maximum allowable runtime.</p></li>
<li><p>Preferred feature selection strategies to include in the
ensemble.</p></li>
<li><p>Tolerance levels for correlation or redundancy among selected
features, and more.</p></li>
</ul>
<p>The approach is also data-agnostic, making no assumptions about the
structure, scale, or domain of the input data. This enables it to be
applied across disciplines, from healthcare to finance to
engineering.</p>
<p>In summary, the proposed system seeks to provide a practical,
powerful, and adaptable solution to one of machine learning’s most
enduring challenges—efficient and effective feature selection in
high-dimensional spaces—through a smart, regulated, and iterative
ensemble pipeline.</p>
<h1 id="related-works">Related Works</h1>
<p>Ensemble Feature Selection (EFS) is a family of techniques that aim
to increase stability and accuracy by aggregating the results of
multiple feature selection methods. The motivation is similar to
ensemble learning in classification: reducing variance and bias by
combining diverse perspectives <span class="citation"
data-cites="saeys2008review"></span>.</p>
<h2 id="efs-ensemble-feature-selection-framework">EFS (Ensemble Feature
Selection Framework)</h2>
<p>A software tool that integrates the results of eight feature
selection algorithms (e.g., ReliefF, SVM-RFE, Random Forest importance)
by normalizing and aggregating their feature rankings or scores <span
class="citation" data-cites="neumann2017efs"></span></p>
<p>Pros:</p>
<ul>
<li><p>Increases robustness against noise or data
perturbations.</p></li>
<li><p>Outperforms individual methods in accuracy and
stability.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Requires careful normalization of output scores.</p></li>
<li><p>Can suffer if constituent methods are too similar or
biased.</p></li>
</ul>
<h2 id="efsis-ensemble-feature-selection-integrating-stability">EFSIS
(Ensemble Feature Selection Integrating Stability)</h2>
<p>Combines two strategies:</p>
<ul>
<li><p>Function perturbation: Using various selection
algorithms.</p></li>
<li><p>Data perturbation: Applies each method on multiple bootstrapped
datasets.</p></li>
</ul>
<p>Then it aggregates the frequency or consistency of each feature’s
selection <span class="citation" data-cites="zhang2018efsis"></span>.
Pros:</p>
<ul>
<li><p>Focuses explicitly on stability, a known weakness of many
selection algorithms.</p></li>
<li><p>Empirically improves robustness and generalization across
datasets.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Higher computational cost (due to multiple runs per
method).</p></li>
<li><p>Parameter tuning (e.g., number of resamples) can affect the
results.</p></li>
</ul>
<h2 id="fs-msi-feature-selection-via-multiple-score-integration">FS-MSI
(Feature Selection via Multiple Score Integration)</h2>
<p>This method assigns each feature a unified score by integrating
scores from multiple traditional methods. Instead of raw ranks, it uses
weighted averages or voting mechanisms <span class="citation"
data-cites="bhattacharyya2017efsmi"></span>. Pros:</p>
<ul>
<li><p>Captures complementary insights from various methods.</p></li>
<li><p>Demonstrates consistent performance gains in classification
tasks.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Weights and scoring strategies must be tuned.</p></li>
<li><p>Lacks model-specific adaptation unless combined with downstream
learning.</p></li>
</ul>
<h2 id="hyb-efs-hybrid-ensemble-feature-selection">Hyb-EFS (Hybrid
Ensemble Feature Selection) </h2>
<p>Combines homogeneous ensembles (e.g., using the same selection
algorithm on different resampled datasets) and heterogeneous ensembles
(e.g., different algorithms). Originally proposed for genomics data
<span class="citation" data-cites="colombelli2021hybrid"></span>.</p>
<p>Pros:</p>
<ul>
<li><p>High reproducibility across biomedical datasets.</p></li>
<li><p>More resilient to both model variance and data
perturbation.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Complex to implement and evaluate.</p></li>
<li><p>May require domain knowledge to tune ensemble
composition.</p></li>
</ul>
<p>Iterative and Adaptive Selection Techniques:</p>
<p>Recent research also focuses on adaptive and iterative pipelines that
refine feature selection over time <span class="citation"
data-cites="feurer2015efficient"></span>.</p>
<h2 id="stability-selection">Stability Selection</h2>
<p>Introduced by Meinshausen and Bühlmann, this method combines LASSO
with subsampling. It selects features that consistently appear across
subsamples and penalization strengths <span class="citation"
data-cites="meinshausen2010stability"></span></p>
<p>Pros:</p>
<ul>
<li><p>Theoretical guarantees on controlling false positives.</p></li>
<li><p>Robust to overfitting.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Relatively slow.</p></li>
<li><p>May miss weak but important features.</p></li>
</ul>
<h2 id="boruta">Boruta</h2>
<p>A Random Forest-based wrapper method that compares the importance of
real features to that of “shadow features” (randomly permuted copies)
<span class="citation" data-cites="kursa2010boruta"></span>. Pros:</p>
<ul>
<li><p>All-relevant feature selection (not just
minimal-optimal).</p></li>
<li><p>Works well for high-dimensional data like genomics.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Computationally heavy.</p></li>
<li><p>Tends to retain many correlated features.</p></li>
</ul>
<h2 id="recursive-feature-addition-rfa">Recursive Feature Addition
(RFA)</h2>
<p>Starts with an empty set and adds features one-by-one, evaluating
model performance (e.g., using cross-validation) at each iteration.
Pros:</p>
<ul>
<li><p>Controlled and explainable process.</p></li>
<li><p>Can terminate early based on runtime or performance
thresholds.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Sensitive to early feature choices.</p></li>
<li><p>Slower than filter-based methods.</p></li>
</ul>
<div id="tab:example">
<table>
<caption>Summary of Trade-Offs</caption>
<thead>
<tr>
<th style="text-align: center;"><strong>Method Type</strong></th>
<th style="text-align: center;"><strong>Pros</strong></th>
<th style="text-align: center;"><strong>Cons</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Filter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">scalable,</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">general</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">interactions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Wrapper</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">accuracy,</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">interaction-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">aware</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">overfitting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">risk</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"><span class="citation"
data-cites="tang2014feature"></span></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Embedded</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">integrated</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">specific</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ensemble</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">robust,</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">hybrid</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">computat-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ionally</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">intensive</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Iterative</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">controllable</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">requires-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">tuning</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:example" data-label="tab:example"></span></p>
<h2 id="relevance-to-our-work">Relevance to Our Work</h2>
<p>This project builds upon the ideas of ensemble and iterative
selection, proposing a smart, regulated, and adaptive ensemble pipeline
that:</p>
<ul>
<li><p>Combines diverse selection strategies.</p></li>
<li><p>Iteratively refines the feature set using the best available
model at each stage.</p></li>
<li><p>Respect user constraints like maximum runtime, number/percentage
of features, or memory budget.</p></li>
<li><p>It is fully data-agnostic and adaptable to any tabular
dataset.</p></li>
</ul>
<p>This hybrid approach is designed to deliver high-quality, explainable
feature subsets — balancing interpretability, efficiency, and
performance. <span class="citation"
data-cites="garcia2020big"></span></p>
<h1 id="data-overview">Data Overview</h1>
<p>The SmartSelect framework is designed to be data-agnostic, capable of
operating on a wide range of datasets without requiring prior domain
knowledge. It is tailored for tabular datasets, where each sample is
represented by multiple features (columns) and a single target column
<span class="citation" data-cites="shang2021tabular"></span>. The
framework supports both regression and classification problems, as long
as they are supervised, meaning that ground-truth labels for the target
variable are provided.</p>
<p>In our experiments, we evaluated SmartSelect on multiple datasets
exhibiting varying characteristics such as dimensionality, class
imbalance, and data sparsity. The datasets ranged in size from small to
large-scale, including those with tens of thousands of features and
thousands of samples, to simulate realistic high-dimensional learning
scenarios.</p>
<p>Due to repository constraints, actual datasets were not included;
however, the system includes placeholders and compatible data loaders to
allow seamless integration of external tabular datasets from .csv
files.</p>
<h1 id="methodology">Methodology</h1>
<h2 id="user-configuration">User Configuration</h2>
<p>At the core of the SmartSelect framework is user flexibility. The
user is required to define the maximum computational complexity they are
willing to tolerate for both the feature selection phase and for the
final benchmark model (described in detail below). This configuration
enables SmartSelect to adjust its internal operations to the user’s time
or resource constraints.</p>
<h2 id="determining-the-number-of-selected-features">Determining the
Number of Selected Features</h2>
<p>Users can explicitly specify the desired number of features to be
selected, either as an absolute value or as a percentage of the original
feature set. If no specification is provided, SmartSelect defaults to
selecting a number of features equal to the square root of the number of
data samples.</p>
<h2 id="feature-selection-strategy">Feature Selection Strategy</h2>
<p>A predefined list of feature selection methods is constructed,
ordered according to their perceived quality. Since the quality of
feature selection methods can be subjective and context-dependent <span
class="citation" data-cites="bolon2015recent"></span>, SmartSelect
provides a default ranking which the user can later modify.</p>
<p>The framework then iteratively evaluates each method in the list
against the user-defined computational complexity threshold:</p>
<ol>
<li><p>If a method is deemed feasible within the given complexity limit,
it is executed.</p></li>
<li><p>The resulting set of selected features is used to filter the
dataset.</p></li>
<li><p>The method is removed from the current iteration list.</p></li>
<li><p>The system re-evaluates the remaining methods—this time using the
reduced dataset—to check whether they now meet the complexity
constraint.</p></li>
</ol>
<p>If a method exceeds the allowed complexity, the system skips it and
proceeds to the next one. If no remaining method is able to run under
the current constraints (or none are left), a new iteration begins with
the full set of predefined methods. However, this time the input is the
reduced dataset output from the previous iteration.</p>
<p>To prevent trivial or ineffective filtering, each feature selection
method is required to retain at least:</p>
<ul>
<li><p>50% of the features it receives as input, and-</p></li>
<li><p>No fewer features than the final target number specified by the
user (if defined).</p></li>
</ul>
<h2 id="stopping-criteria">Stopping Criteria</h2>
<p>The iterative selection process stops when any of the following
conditions are met:</p>
<ol>
<li><p>The number of features reaches the user-defined target.</p></li>
<li><p>The number of features is sufficiently reduced to enable running
the benchmark model within the specified complexity limit.</p></li>
<li><p>None of the methods in the current iteration meet the runtime
constraint. In this case, the system will stop and prompt the user to
increase the allowed complexity.</p></li>
<li><p>The feature selection methods no longer perform additional
filtering, yet the dimensionality is still too high to run the benchmark
model. The system will again prompt the user to relax the complexity
constraint of the benchmark model.</p></li>
</ol>
<h2 id="benchmark-model">Benchmark Model</h2>
<p>The benchmark model is designed to act as an independent,
high-quality evaluator of the final feature sets.</p>
<p>Once the feature space is reduced to a level that permits execution
of the benchmark model within the user-defined complexity constraint,
SmartSelect identifies all feature selection methods that are
computationally feasible at this stage. These methods are then
re-executed, each constrained to produce a feature subset whose
dimensionality exactly matches the target specified by the user—whether
defined as an absolute number or a percentage.</p>
<p>Each selected subset is then evaluated using a strong machine
learning model—typically a boosting-based model—trained and tested using
a train-test split. The performance of each subset is evaluated using an
appropriate metric (e.g., RMSE for regression or F1 score for
classification).</p>
<p>The subset that achieves the best score on the test set is selected
as the final feature set output by SmartSelect.</p>
<p><span class="citation" data-cites="chen2016xgboost"></span></p>
<h2 id="feature-selection-workflow-diagram">Feature Selection Workflow
Diagram</h2>
<figure id="fig:feature_selection_flow_grid">
<img src="images/feature_selection_flow_grid.png" style="width:70.0%" />
<figcaption>Illustrates the iterative feature selection process employed
by SmartSelect, incorporating user-defined constraints and dynamic
method evaluation.</figcaption>
</figure>
<h1 id="results">Results</h1>
<h2 id="evaluation-procedure">Evaluation Procedure</h2>
<p>To assess the performance of the SmartSelect framework, we tested it
across multiple tabular datasets encompassing both regression and
classification problems. Each dataset was divided into training and test
sets. For each experiment, we compared the performance of our method
against a traditional feature selection technique, applied
individually.</p>
<p>For each dataset, an XGBoost model was trained on the training data
and evaluated on the test set. The performance metric depended on the
task type: F1 score for classification and mean squared error (MSE) for
regression.</p>
<h2 id="performance-comparison">Performance Comparison</h2>
<p>Across all experiments, SmartSelect consistently outperformed
individual feature selection methods in predictive performance on the
test set <span class="citation" data-cites="guyon2006feature"></span>,
while maintaining reasonable runtime.</p>
<p>The most competitive baseline was observed on the SCANB dataset for a
regression task with Lympho column as the label. Over 20 independent
runs of SmartSelect, we obtained a mean squared error (MSE) of 0.00784,
with a standard deviation of 0.00026 and a maximum value of 0.00833.</p>
<p>In comparison, the Variance Threshold method (It only ran once, since
it’s a deterministic method) resulted in an MSE of 0.01242.</p>
<h2 id="example-results">Example Results</h2>
<p>The figure below presents a representative result comparing
SmartSelect with a baseline feature selection method on the SCANB
dataset. Target: PAM50 — LumA vs. others.</p>
<p><strong>All evaluation results, including this example and full
performance distributions across all datasets, are provided in the
Appendix.</strong></p>
<p><strong>SCANB data, Target = PAM50</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Binary Classification (Target: PAM50 — LumA vs. others)</p></li>
<li><p>Average running time of SmartSelect:  10 minutes</p></li>
</ul>
<figure id="fig:PAM_50_1">
<img src="images/PAM_50_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_2">
<img src="images/PAM_50_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_3">
<img src="images/PAM_50_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<h1 id="discussion-and-conclusion">Discussion and Conclusion</h1>
<p>This project introduced SmartSelect, an adaptive and modular
framework for feature selection in high-dimensional tabular datasets. By
integrating multiple selection techniques in a controlled iterative
process, SmartSelect balances accuracy and computational efficiency
while requiring minimal user intervention. The system allows users to
define constraints such as the number of desired features and complexity
budgets, enabling flexible deployment across various domains.</p>
<p><strong>Experimental results consistently demonstrated that
SmartSelect outperforms traditional single-method approaches in both
regression and classification tasks. The framework achieved lower mean
squared error in regression and higher F1 scores in classification
across multiple real-world datasets. These gains were especially notable
in high-dimensional scenarios</strong>- where conventional methods
either failed to scale or yielded suboptimal results <span
class="citation" data-cites="li2017feature"></span> —
<strong>highlighting SmartSelect’s core advantage in handling large
feature spaces.</strong></p>
<p><strong>Another notable benefit of SmartSelect is its increased
result stability: across repeated runs, the standard deviation of
performance metrics (such as MSE and F1) was significantly lower
compared to other methods.</strong> This reduced variance suggests
greater robustness to stochastic effects in the selection process (e.g.,
train-test splits or randomized algorithms), which is especially
important in real-world pipelines where reproducibility and reliability
are critical.</p>
<p>A key advantage of SmartSelect lies in its full automation and
configurability. Without tuning internal hyperparameters, the system
effectively navigates the trade-off between runtime and performance. The
ability to adaptively filter features based on dynamic complexity
constraints makes it especially suitable for real-world applications,
where resource limitations are common.</p>
<p>A graphical example from a single run of the system is provided
below, illustrating the step-by-step feature selection process over
multiple iterations. While the time complexities of individual steps may
appear high in the plot, it is important to note that the times are
normalized relative to the size of the dataset at that point. As the
number of features decreases throughout the process, subsequent
selection methods run significantly faster, making the overall runtime
more efficient than it may initially seem in the graph.</p>
<figure id="fig:feature_selection_progression_example">
<img src="images/feature_selection_progression_example.png"
style="width:70.0%" />
<figcaption>Feature Selection Progression over Iterations -
example</figcaption>
</figure>
<p>Overall, SmartSelect presents a practical, efficient, and scalable
solution to the feature selection problem, particularly in environments
where high dimensionality and runtime sensitivity pose challenges.</p>
<h1 id="future-work">Future Work</h1>
<p>While SmartSelect demonstrates strong performance across a variety of
supervised learning tasks, several important extensions remain for
future research and development.</p>
<h2 id="support-for-unsupervised-learning">Support for Unsupervised
Learning</h2>
<p>Currently, SmartSelect is designed for supervised scenarios, where
ground-truth labels allow for clear evaluation metrics and benchmarking.
Extending the framework to support unsupervised learning tasks presents
a unique challenge, primarily due to the absence of a definitive ground
truth to guide feature evaluation.</p>
<p>Future versions of the system would need to rely on intrinsic
evaluation metrics (e.g., silhouette score, reconstruction error) or
proxy objectives to assess feature quality in unsupervised contexts.
Moreover, the internal selection mechanisms and benchmark model
architecture would need to be adapted to operate in unsupervised
pipelines.</p>
<h2 id="supervised-anomaly-detection">Supervised Anomaly Detection</h2>
<p>Another future task is adapting SmartSelect for supervised anomaly
detection tasks. The current system is optimized for traditional
supervised prediction, where average performance across the dataset
(e.g., mean squared error or classification accuracy) is the target. In
such settings, features that are highly correlated with each other are
often considered redundant in classical prediction models and the system
is encouraged to retain only one.</p>
<p>However, in supervised anomaly detection, this assumption may not
hold. Correlated features might capture subtle, rare variations in the
data that are critical for identifying anomalies <span class="citation"
data-cites="zimek2012survey"></span>. For example, two features that are
99% correlated may still exhibit meaningful divergence in the tail of
their distribution—precisely the region of interest in anomaly
detection.</p>
<p>Supporting this use case would require modifying the selection
strategy to preserve redundant but informative patterns, and potentially
developing new evaluation criteria that reflect anomaly-specific utility
rather than average-case accuracy.</p>
<h2 id="multiclass-classification-support">Multiclass Classification
Support</h2>
<p>Currently, SmartSelect works in binary classification and regression
settings. Extending the framework to better support multiclass
classification would involve adjustments in both evaluation metrics
(e.g., macro/micro-averaged F1 scores) and in the benchmark model’s
architecture and selection thresholds.</p>
<h2 id="parallel-execution-of-selection-methods">Parallel Execution of
Selection Methods</h2>
<p>The current implementation evaluates feature selection methods
sequentially. However, in the benchmark stage—multiple methods could be
evaluated in parallel without exceeding runtime constraints.
Incorporating parallel execution for compatible methods in the benchmark
phase could significantly reduce overall processing time.</p>
<h2 id="user-control-over-method-parameters-and-regularization">User
Control Over Method Parameters and Regularization</h2>
<p>In its current form, SmartSelect uses fixed thresholds for method
behavior, such as the minimum number of features retained. Providing the
user with greater control over parameters and regularization
constraints, such as specifying the minimum or maximum percentage of
features each method is allowed to retain, would make the system more
adaptable to different use cases and domain-specific requirements.</p>
<h2 id="support-for-non-numerical-features">Support for Non-Numerical
Features</h2>
<p>SmartSelect currently assumes that all input features have been
preprocessed into numerical form. However, the transformation of
categorical features can have a major impact on the relevance and
behavior of feature selection methods—especially in contexts like
anomaly detection, where rare values can carry critical information.</p>
<p>Future versions of the system could incorporate preprocessing
strategies such as adaptive one-hot encoding, guided by feature
cardinality or class imbalance, as an integral part of the selection
process rather than a preprocessing step external to the system.</p>
<h1 id="appendix">Appendix</h1>
<h2 id="full-results-across-all-five-datasets">Full Results Across All
Five Datasets</h2>
<p>This appendix presents the complete results for all five datasets
evaluated in our experiments, including the dataset shown in the main
paper.</p>
<p><strong>SCANB data, Target = PAM50</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Binary Classification (Target: PAM50 — LumA vs. others)</p></li>
<li><p>Average running time of SmartSelect:  10 minutes</p></li>
</ul>
<figure id="fig:PAM_50_1">
<img src="images/PAM_50_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_2">
<img src="images/PAM_50_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_3">
<img src="images/PAM_50_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<p><strong>SCANB data, Target = ER</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Binary classification task (Target: ER)</p></li>
<li><p>Average running time of SmartSelect:  13 minutes</p></li>
</ul>
<figure id="fig:SCANB_class_1">
<img src="images/SCANB_class_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = ER</figcaption>
</figure>
<figure id="fig:SCANB_class_2">
<img src="images/SCANB_class_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = ER</figcaption>
</figure>
<figure id="fig:SCANB_class_3">
<img src="images/SCANB_class_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = ER</figcaption>
</figure>
<p><strong>SCANB data, Target = Lympho</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Regression task (Target: Lympho)</p></li>
<li><p>Average running time of SmartSelect:  20 minutes</p></li>
</ul>
<figure id="fig:SCANB_reg_1">
<img src="images/SCANB_reg_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = Lympho</figcaption>
</figure>
<figure id="fig:SCANB_reg_2">
<img src="images/SCANB_reg_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = Lympho</figcaption>
</figure>
<figure id="fig:SCANB_reg_3">
<img src="images/SCANB_reg_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = Lympho</figcaption>
</figure>
<p><strong>MNIST 784</strong></p>
<ul>
<li><p>(14,780 rows × 784 columns)</p></li>
<li><p>Binary Classification</p></li>
<li><p>Average running time of SmartSelect:  0.5 minutes</p></li>
<li><p><strong>Note: Given the relatively low feature dimensionality of
this dataset, performance differences between methods were smaller. The
advantages of SmartSelect are more pronounced in high-dimensional
settings.</strong></p></li>
</ul>
<figure id="fig:MNIST_1">
<img src="images/MNIST_1.png" style="width:70.0%" />
<figcaption>MNIST 784</figcaption>
</figure>
<figure id="fig:MNIST_2">
<img src="images/MNIST_2.png" style="width:70.0%" />
<figcaption>MNIST 784</figcaption>
</figure>
<figure id="fig:MNIST_3">
<img src="images/MNIST_3.png" style="width:70.0%" />
<figcaption>MNIST 784</figcaption>
</figure>
<p><strong>GSE184773 data</strong></p>
<ul>
<li><p>(24 rows × 24,057 columns)</p></li>
<li><p>Binary Classification</p></li>
<li><p>Average running time of SmartSelect:   5 seconds</p></li>
</ul>
<figure id="fig:GSE184773_1">
<img src="images/GSE184773_1.png" style="width:70.0%" />
<figcaption>GSE184773 data</figcaption>
</figure>
<figure id="fig:GSE184773_2">
<img src="images/GSE184773_2.png" style="width:70.0%" />
<figcaption>GSE184773 data</figcaption>
</figure>
<figure id="fig:GSE184773_3">
<img src="images/GSE184773_3.png" style="width:70.0%" />
<figcaption>GSE184773 data</figcaption>
</figure>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>SmartSelect — Feature Selection Framework</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      color: #333;
      margin: 0;
      background-color: #f5f7fa;
      line-height: 1.7;
    }
    header {
      background: #2c3e50;
      color: #fff;
      padding: 2rem 1rem;
      text-align: center;
    }
    header h1 { margin: 0; font-size: 2.2rem; }
    header p { margin-top: 0.5rem; font-size: 1rem; }
    nav {
      background: #2980b9;
      padding: 0.5rem 1rem;
        position: sticky;
  top: 0;
  z-index: 1000;
    }

    nav a {
      color: #fff;
      margin-right: 1rem;
      text-decoration: none;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    main {
      max-width: 900px;
      background: #fff;
      margin: -1rem auto 2rem;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 0 15px rgba(0,0,0,0.1);
    }
    section {
  margin-bottom: 2rem;
  scroll-margin-top: 100px; 
  }
    h2 { scroll-margin-top: 120px; border-bottom: 2px solid #e1e4e8; padding-bottom: 0.4rem; color: #2c3e50; }
    p { margin: 1rem 0; }
    ul { margin: 1rem 0 1rem 1.5rem; }
    code {
      background: #f4f6f8;
      padding: 2px 5px;
      border-radius: 4px;
      font-family: monospace;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
    }
    caption {
      font-size: 1.2rem;
      font-weight: bold;
      margin-bottom: 0.5rem;
      color: #34495e;
    }
    th, td {
      border: 1px solid #e1e4e8;
      padding: 12px;
      text-align: center;
    }
    th {
      background: #2980b9;
      color: #fff;
    }
    tr:hover {
      background: #ecf0f1;
    }
    img {
      max-width: 100%;
      display: block;
      margin: 1rem auto;
      border-radius: 4px;
      box-shadow: 0 0 8px rgba(0,0,0,0.1);
    }
    footer {
      text-align: center;
      padding: 2rem;
      font-size: 0.9rem;
      color: #888;
    }
.nav-button {
  display: inline-block;
  background-color: #2980b9;
  color: #fff;
  padding: 0.5rem 1rem;
  border-radius: 6px;
  text-decoration: none;
  font-weight: bold;
  margin-top: 1rem;
  transition: background-color 0.3s ease, text-decoration 0.3s ease;
}

.nav-button:hover {
  background-color: #2471a3; 
  text-decoration: underline;
}
  </style>
</head>
<body>

  <header>
    <h1>SmartSelect</h1>
    <p>Feature Selection for High-Dimensional Data</p>
    <p><a href="https://www.linkedin.com/in/yehuda-baharav/"  style="color:#ffffff"><strong>Yehuda Baharav</strong></a></p>
<p>
<a href="paper.pdf" class="nav-button" target="_blank">
  📄 View Full Academic Paper (PDF)
</a>
</p>
  </header>

  <nav>
    <a href="#introduction">Introduction</a>
    <a href="#related_works">Related Works</a>
    <a href="#data_overview">Data Overview</a>
    <a href="#methodology">Methodology</a>
    <a href="#results">Results</a>
    <a href="#discussion_and_conclusion">Discussion and Conclusion</a>
    <a href="#future_work">Future Work</a>
    <a href="#appendix">Appendix</a>
  </nav>

  <main>
    <section id="introduction">
      <h2>1. Introduction</h2>
      <p>In today’s data-driven landscape, machine
learning models are frequently tasked with
analyzing high-dimensional datasets—those
containing hundreds, thousands, or even millions of features. This challenge is particularly pronounced in domains such as
genomics, cybersecurity, image and speech
recognition, and general data science, where
data often includes a vast number of variables
relative to the number of observations.</p>
      
<p>While having access to a large number of features might seem advantageous, in practice,
it often introduces serious complications. Redundant, irrelevant, or noisy features can obscure the underlying patterns in the data,
leading to decreased predictive performance.
Additionally, they significantly increase the
computational burden during model training
and inference, which is particularly problematic when working under constraints of time
or limited computing resources. In extreme cases, the presence of such features can
even mislead the model, causing it to learn
spurious correlations instead of meaningful
insights.</p>

<p>Feature selection—a critical preprocessing
step in the machine learning pipeline—aims
to address these challenges. By identifying
and retaining only the most relevant and informative features, feature selection can improve model accuracy, reduce overfitting, accelerate training, and enhance interpretability. However, no single feature selection technique works best in all scenarios.
Some methods are statistically rigorous but
computationally intensive, making them impractical for large-scale datasets. Others are
fast and scalable but may compromise on selection quality, potentially overlooking subtle
but important variables.</p>

<p>This project introduces a smart ensemble approach to feature selection, which combines
the strengths of multiple existing methods
to achieve a robust balance between performance (in terms of selection quality and resulting model accuracy) and efficiency (in
terms of runtime and resource usage). Ensemble feature selection has been shown to
improve stability and accuracy by aggregating results from multiple base methods.</p>

<p>A key innovation of this approach lies in its
iterative pipeline structure. Instead of performing feature selection in a single pass,
the system proceeds through multiple controlled iterations. In each iteration, it evaluates the feature set using the best available model—determined dynamically based
on current constraints and performance metrics. These constraints, or regulations, include user-defined parameters such as maximum runtime, memory limits, or a cap on
the number of features. The model choice at
each step is adaptive: for example, the system
might choose a simple and fast model early
in the process to rapidly eliminate uninformative features, and then refine the selection
using a more complex and accurate model as
the feature space becomes smaller and more
manageable.</p>

<p>This regulated, adaptive, and iterative process ensures that the pipeline remains both
efficient and effective across a wide range of
use cases and datasets. It continuously optimizes feature selection quality while respecting practical constraints, striking an optimal
trade-off between speed and accuracy.</p>

<p>Furthermore, the implementation offers a
high degree of user customization and control. Users can specify a range of parameters
according to their needs and constraints, including:
<ul>
  <li>The target number of features (either as
an exact number or a percentage of the
original set).</li>
  <li>Maximum allowable runtime.</li>
  <li>Preferred feature selection strategies to
include in the ensemble.</li>
   <li>Tolerance levels for correlation or redundancy among selected features, and
more.</li>
</ul>
</p>

<p>The approach is also data-agnostic, making
no assumptions about the structure, scale, or
domain of the input data. This enables it to
be applied across disciplines, from healthcare
to finance to engineering.</p>

<p>In summary, the proposed system seeks to
provide a practical, powerful, and adaptable solution to one of machine learning’s
most enduring challenges—efficient and effective feature selection in high-dimensional
spaces—through a smart, regulated, and iterative ensemble pipeline.</p>
    </section>

    <section id="related_works">
      <h2>2. Related Works</h2>
      <p>TEnsemble Feature Selection (EFS) is a family of techniques that aim to increase stability and accuracy by aggregating the results
of multiple feature selection methods. The
motivation is similar to ensemble learning in
classification: reducing variance and bias by
combining diverse perspectives</p>
      
      <h3>EFS (Ensemble Feature Selection Framework)</h3>
<p>
  A software tool that integrates the results of
eight feature selection algorithms (e.g., ReliefF, SVM-RFE, Random Forest importance)
by normalizing and aggregating their feature
rankings or scores
</p>
      <p>Pros:</p>
<ul>
<li><p>Increases robustness against noise or data
perturbations.</p></li>
<li><p>Outperforms individual methods in accuracy and
stability.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Requires careful normalization of output scores.</p></li>
<li><p>Can suffer if constituent methods are too similar or
biased.</p></li>
</ul>

<h3>EFSIS (Ensemble Feature
Selection Integrating Stability)</h3>

<p>Combines two strategies:</p>
<ul>
<li><p>Function perturbation: Using various selection
algorithms.</p></li>
<li><p>Data perturbation: Applies each method on multiple bootstrapped
datasets.</p></li>
</ul>
<p>Then it aggregates the frequency or consistency of each feature’s
selection.
Pros:</p>
<ul>
<li><p>Focuses explicitly on stability, a known weakness of many
selection algorithms.</p></li>
<li><p>Empirically improves robustness and generalization across
datasets.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Higher computational cost (due to multiple runs per
method).</p></li>
<li><p>Parameter tuning (e.g., number of resamples) can affect the
results.</p></li>
</ul>      
<h3>FS-MSI
(Feature Selection via Multiple Score Integration)</h3>
<p>This method assigns each feature a unified score by integrating
scores from multiple traditional methods. Instead of raw ranks, it uses
weighted averages or voting mechanisms. 
</p>
      <p>Pros:</p>
<ul>
<li><p>Captures complementary insights from various methods.</p></li>
<li><p>Demonstrates consistent performance gains in classification
tasks.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Weights and scoring strategies must be tuned.</p></li>
<li><p>Lacks model-specific adaptation unless combined with downstream
learning.</p></li>
</ul>
<h3>Hyb-EFS (Hybrid
Ensemble Feature Selection) </h3>
<p>Combines homogeneous ensembles (e.g., using the same selection
algorithm on different resampled datasets) and heterogeneous ensembles
(e.g., different algorithms). Originally proposed for genomics data
.</p>
<p>Pros:</p>
<ul>
<li><p>High reproducibility across biomedical datasets.</p></li>
<li><p>More resilient to both model variance and data
perturbation.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Complex to implement and evaluate.</p></li>
<li><p>May require domain knowledge to tune ensemble
composition.</p></li>
</ul>
<p>Iterative and Adaptive Selection Techniques:</p>
<p>Recent research also focuses on adaptive and iterative pipelines that
refine feature selection over time.</p>
<h3>Stability Selection</h3>
<p>Introduced by Meinshausen and Bühlmann, this method combines LASSO
with subsampling. It selects features that consistently appear across
subsamples and penalization strengths </span></p>
<p>Pros:</p>
<ul>
<li><p>Theoretical guarantees on controlling false positives.</p></li>
<li><p>Robust to overfitting.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Relatively slow.</p></li>
<li><p>May miss weak but important features.</p></li>
</ul>
<h3>Boruta</h3>
<p>A Random Forest-based wrapper method that compares the importance of
real features to that of “shadow features” (randomly permuted copies)
</span>.</p> 
<p></p>Pros:</p>
<ul>
<li><p>All-relevant feature selection (not just
minimal-optimal).</p></li>
<li><p>Works well for high-dimensional data like genomics.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Computationally heavy.</p></li>
<li><p>Tends to retain many correlated features.</p></li>
</ul>
<h3>Recursive Feature Addition
(RFA)</h3>
<p>Starts with an empty set and adds features one-by-one, evaluating
model performance (e.g., using cross-validation) at each iteration.
Pros:</p>
<ul>
<li><p>Controlled and explainable process.</p></li>
<li><p>Can terminate early based on runtime or performance
thresholds.</p></li>
</ul>
<p>Cons:</p>
<ul>
<li><p>Sensitive to early feature choices.</p></li>
<li><p>Slower than filter-based methods.</p></li>
</ul>



      <h3>Summary of Trade-Offs</h3>
      <table>
        <thead>
          <tr>
            <th>Method Type</th><th>Pros</th><th>Cons</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>Filter</td><td>Scalable, general-purpose</td><td>Ignores interactions</td></tr>
          <tr><td>Wrapper</td><td>Accurate, interaction-aware</td><td>Overfitting risk, slow</td></tr>
          <tr><td>Embedded</td><td>Model-integrated, specific</td><td>Sensitive to early choices</td></tr>
          <tr><td>Ensemble</td><td>Robust, hybrid</td><td>Compute-heavy</td></tr>
          <tr><td>Iterative</td><td>Controllable</td><td>Requires tuning</td></tr>
        </tbody>
      </table>

<h3>Relevance to Our Work</h3>
<p>This project builds upon the ideas of ensemble and iterative
selection, proposing a smart, regulated, and adaptive ensemble pipeline
that:</p>
<ul>
<li><p>Combines diverse selection strategies.</p></li>
<li><p>Iteratively refines the feature set using the best available
model at each stage.</p></li>
<li><p>Respect user constraints like maximum runtime, number/percentage
of features, or memory budget.</p></li>
<li><p>It is fully data-agnostic and adaptable to any tabular
dataset.</p></li>
</ul>
<p>This hybrid approach is designed to deliver high-quality, explainable
feature subsets — balancing interpretability, efficiency, and
performance.</p>

    </section>

    <section id="data_overview">
      <h2>3. Data Overview</h2>
    <p>The SmartSelect framework is designed to be data-agnostic, capable of
operating on a wide range of datasets without requiring prior domain
knowledge. It is tailored for tabular datasets, where each sample is
represented by multiple features (columns) and a single target column
. 
The
framework supports both regression and classification problems, as long
as they are supervised, meaning that ground-truth labels for the target
variable are provided.</p>
<p>In our experiments, we evaluated SmartSelect on multiple datasets
exhibiting varying characteristics such as dimensionality, class
imbalance, and data sparsity. The datasets ranged in size from small to
large-scale, including those with tens of thousands of features and
thousands of samples, to simulate realistic high-dimensional learning
scenarios.</p>
<p>Due to repository constraints, actual datasets were not included;
however, the system includes placeholders and compatible data loaders to
allow seamless integration of external tabular datasets from .csv
files.</p>

    </section>

    <section id="methodology">
          <h2>4. Methodology</h2>
<h3>User Configuration</h3>
<p>At the core of the SmartSelect framework is user flexibility. The
user is required to define the maximum computational complexity they are
willing to tolerate for both the feature selection phase and for the
final benchmark model (described in detail below). This configuration
enables SmartSelect to adjust its internal operations to the user’s time
or resource constraints.</p>
<h3>Determining the
Number of Selected Features</h3>
<p>Users can explicitly specify the desired number of features to be
selected, either as an absolute value or as a percentage of the original
feature set. If no specification is provided, SmartSelect defaults to
selecting a number of features equal to the square root of the number of
data samples.</p>
<h3>Feature Selection Strategy</h3>
<p>A predefined list of feature selection methods is constructed,
ordered according to their perceived quality. Since the quality of
feature selection methods can be subjective and context-dependent, SmartSelect
provides a default ranking which the user can later modify.</p>
<p>The framework then iteratively evaluates each method in the list
against the user-defined computational complexity threshold:</p>
<ol>
<li><p>If a method is deemed feasible within the given complexity limit,
it is executed.</p></li>
<li><p>The resulting set of selected features is used to filter the
dataset.</p></li>
<li><p>The method is removed from the current iteration list.</p></li>
<li><p>The system re-evaluates the remaining methods—this time using the
reduced dataset—to check whether they now meet the complexity
constraint.</p></li>
</ol>
<p>If a method exceeds the allowed complexity, the system skips it and
proceeds to the next one. If no remaining method is able to run under
the current constraints (or none are left), a new iteration begins with
the full set of predefined methods. However, this time the input is the
reduced dataset output from the previous iteration.</p>
<p>To prevent trivial or ineffective filtering, each feature selection
method is required to retain at least:</p>
<ul>
<li><p>50% of the features it receives as input, and-</p></li>
<li><p>No fewer features than the final target number specified by the
user (if defined).</p></li>
</ul>
<h3>Stopping Criteria</h3>
<p>The iterative selection process stops when any of the following
conditions are met:</p>
<ol>
<li><p>The number of features reaches the user-defined target.</p></li>
<li><p>The number of features is sufficiently reduced to enable running
the benchmark model within the specified complexity limit.</p></li>
<li><p>None of the methods in the current iteration meet the runtime
constraint. In this case, the system will stop and prompt the user to
increase the allowed complexity.</p></li>
<li><p>The feature selection methods no longer perform additional
filtering, yet the dimensionality is still too high to run the benchmark
model. The system will again prompt the user to relax the complexity
constraint of the benchmark model.</p></li>
</ol>
<h3>Benchmark Model</h3>
<p>The benchmark model is designed to act as an independent,
high-quality evaluator of the final feature sets.</p>
<p>Once the feature space is reduced to a level that permits execution
of the benchmark model within the user-defined complexity constraint,
SmartSelect identifies all feature selection methods that are
computationally feasible at this stage. These methods are then
re-executed, each constrained to produce a feature subset whose
dimensionality exactly matches the target specified by the user—whether
defined as an absolute number or a percentage.</p>
<p>Each selected subset is then evaluated using a strong machine
learning model—typically a boosting-based model—trained and tested using
a train-test split. The performance of each subset is evaluated using an
appropriate metric (e.g., RMSE for regression or F1 score for
classification).</p>
<p>The subset that achieves the best score on the test set is selected
as the final feature set output by SmartSelect.</p>
<h3>Feature Selection Workflow
Diagram</h3>
<figure id="fig:feature_selection_flow_grid", style="text-align: center;">
<img src="images/feature_selection_flow_grid.png" style="width:70.0%" />
<figcaption>Illustrates the iterative feature selection process employed
by SmartSelect, incorporating user-defined constraints and dynamic
method evaluation.</figcaption>
</figure>
          </section>

    <section id="results">
<h2>5. Results</h2>
<h3>Evaluation Procedure</h3>
<p>To assess the performance of the SmartSelect framework, we tested it
across multiple tabular datasets encompassing both regression and
classification problems. Each dataset was divided into training and test
sets. For each experiment, we compared the performance of our method
against a traditional feature selection technique, applied
individually.</p>
<p>For each dataset, an XGBoost model was trained on the training data
and evaluated on the test set. The performance metric depended on the
task type: F1 score for classification and mean squared error (MSE) for
regression.</p>
<h3>Performance Comparison</h3>
<p>Across all experiments, SmartSelect consistently outperformed
individual feature selection methods in predictive performance on the
test set,
while maintaining reasonable runtime.</p>
<p>The most competitive baseline was observed on the SCANB dataset for a
regression task with Lympho column as the label. Over 20 independent
runs of SmartSelect, we obtained a mean squared error (MSE) of 0.00784,
with a standard deviation of 0.00026 and a maximum value of 0.00833.</p>
<p>In comparison, the Variance Threshold method (It only ran once, since
it’s a deterministic method) resulted in an MSE of 0.01242.</p>
<h3>Example Results</h3>
<p>The figure below presents a representative result comparing
SmartSelect with a baseline feature selection method on the SCANB
dataset. Target: PAM50 — LumA vs. others.</p>
<p><strong>All evaluation results, including this example and full
performance distributions across all datasets, are provided in the
Appendix.</strong></p>
<p><strong>SCANB data, Target = PAM50</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Binary Classification (Target: PAM50 — LumA vs. others)</p></li>
<li><p>Average running time of SmartSelect:  10 minutes</p></li>
</ul>
<figure id="fig:PAM_50_1", style="text-align: center;">
<img src="images/PAM_50_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_2", style="text-align: center;">
<img src="images/PAM_50_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_3", style="text-align: center;">
<img src="images/PAM_50_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>

                </section>

    <section id="discussion_and_conclusion">
<h2>6. Discussion and Conclusion</h2>
<p>This project introduced SmartSelect, an adaptive and modular
framework for feature selection in high-dimensional tabular datasets. By
integrating multiple selection techniques in a controlled iterative
process, SmartSelect balances accuracy and computational efficiency
while requiring minimal user intervention. The system allows users to
define constraints such as the number of desired features and complexity
budgets, enabling flexible deployment across various domains.</p>
<p><strong>Experimental results consistently demonstrated that
SmartSelect outperforms traditional single-method approaches in both
regression and classification tasks. The framework achieved lower mean
squared error in regression and higher F1 scores in classification
across multiple real-world datasets. These gains were especially notable
in high-dimensional scenarios</strong>- where conventional methods
either failed to scale or yielded suboptimal results —
<strong>highlighting SmartSelect’s core advantage in handling large
feature spaces.</strong></p>
<p><strong>Another notable benefit of SmartSelect is its increased
result stability: across repeated runs, the standard deviation of
performance metrics (such as MSE and F1) was significantly lower
compared to other methods.</strong> This reduced variance suggests
greater robustness to stochastic effects in the selection process (e.g.,
train-test splits or randomized algorithms), which is especially
important in real-world pipelines where reproducibility and reliability
are critical.</p>
<p>A key advantage of SmartSelect lies in its full automation and
configurability. Without tuning internal hyperparameters, the system
effectively navigates the trade-off between runtime and performance. The
ability to adaptively filter features based on dynamic complexity
constraints makes it especially suitable for real-world applications,
where resource limitations are common.</p>
<p>A graphical example from a single run of the system is provided
below, illustrating the step-by-step feature selection process over
multiple iterations. While the time complexities of individual steps may
appear high in the plot, it is important to note that the times are
normalized relative to the size of the dataset at that point. As the
number of features decreases throughout the process, subsequent
selection methods run significantly faster, making the overall runtime
more efficient than it may initially seem in the graph.</p>
<figure id="fig:feature_selection_progression_example", style="text-align: center;">
<img src="images/feature_selection_progression_example.png"
style="width:70.0%" />
<figcaption>Feature Selection Progression over Iterations -
example</figcaption>
</figure>
<p>Overall, SmartSelect presents a practical, efficient, and scalable
solution to the feature selection problem, particularly in environments
where high dimensionality and runtime sensitivity pose challenges.</p>

                      </section>

    <section id="future_work">
<h2>7. Future Work</h2>
<p>While SmartSelect demonstrates strong performance across a variety of
supervised learning tasks, several important extensions remain for
future research and development.</p>
<h3>Support for Unsupervised
Learning</h3>
<p>Currently, SmartSelect is designed for supervised scenarios, where
ground-truth labels allow for clear evaluation metrics and benchmarking.
Extending the framework to support unsupervised learning tasks presents
a unique challenge, primarily due to the absence of a definitive ground
truth to guide feature evaluation.</p>
<p>Future versions of the system would need to rely on intrinsic
evaluation metrics (e.g., silhouette score, reconstruction error) or
proxy objectives to assess feature quality in unsupervised contexts.
Moreover, the internal selection mechanisms and benchmark model
architecture would need to be adapted to operate in unsupervised
pipelines.</p>
<h3>Supervised Anomaly Detection</h3>
<p>Another future task is adapting SmartSelect for supervised anomaly
detection tasks. The current system is optimized for traditional
supervised prediction, where average performance across the dataset
(e.g., mean squared error or classification accuracy) is the target. In
such settings, features that are highly correlated with each other are
often considered redundant in classical prediction models and the system
is encouraged to retain only one.</p>
<p>However, in supervised anomaly detection, this assumption may not
hold. Correlated features might capture subtle, rare variations in the
data that are critical for identifying anomalies. For example, two features that are
99% correlated may still exhibit meaningful divergence in the tail of
their distribution—precisely the region of interest in anomaly
detection.</p>
<p>Supporting this use case would require modifying the selection
strategy to preserve redundant but informative patterns, and potentially
developing new evaluation criteria that reflect anomaly-specific utility
rather than average-case accuracy.</p>
<h3>Multiclass Classification
Support</h3>
<p>Currently, SmartSelect works in binary classification and regression
settings. Extending the framework to better support multiclass
classification would involve adjustments in both evaluation metrics
(e.g., macro/micro-averaged F1 scores) and in the benchmark model’s
architecture and selection thresholds.</p>
<h3>Parallel Execution of
Selection Methods</h3>
<p>The current implementation evaluates feature selection methods
sequentially. However, in the benchmark stage—multiple methods could be
evaluated in parallel without exceeding runtime constraints.
Incorporating parallel execution for compatible methods in the benchmark
phase could significantly reduce overall processing time.</p>
<h3>User
Control Over Method Parameters and Regularization</h3>
<p>In its current form, SmartSelect uses fixed thresholds for method
behavior, such as the minimum number of features retained. Providing the
user with greater control over parameters and regularization
constraints, such as specifying the minimum or maximum percentage of
features each method is allowed to retain, would make the system more
adaptable to different use cases and domain-specific requirements.</p>
<h3>Support for Non-Numerical
Features</h3>
<p>SmartSelect currently assumes that all input features have been
preprocessed into numerical form. However, the transformation of
categorical features can have a major impact on the relevance and
behavior of feature selection methods—especially in contexts like
anomaly detection, where rare values can carry critical information.</p>
<p>Future versions of the system could incorporate preprocessing
strategies such as adaptive one-hot encoding, guided by feature
cardinality or class imbalance, as an integral part of the selection
process rather than a preprocessing step external to the system.</p>

                            </section>

    <section id="appendix">
<h2>8. Appendix</h2>
<h3>Full Results Across All
Five Datasets</h3>
<p>This appendix presents the complete results for all five datasets
evaluated in our experiments, including the dataset shown in the main
paper.</p>
<p><strong>SCANB data, Target = PAM50</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Binary Classification (Target: PAM50 — LumA vs. others)</p></li>
<li><p>Average running time of SmartSelect:  10 minutes</p></li>
</ul>
<figure id="fig:PAM_50_1", style="text-align: center;">
<img src="images/PAM_50_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_2", style="text-align: center;">
<img src="images/PAM_50_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<figure id="fig:PAM_50_3", style="text-align: center;">
<img src="images/PAM_50_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = PAM50</figcaption>
</figure>
<p><strong>SCANB data, Target = ER</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Binary classification task (Target: ER)</p></li>
<li><p>Average running time of SmartSelect:  13 minutes</p></li>
</ul>
<figure id="fig:SCANB_class_1", style="text-align: center;">
<img src="images/SCANB_class_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = ER</figcaption>
</figure>
<figure id="fig:SCANB_class_2", style="text-align: center;">
<img src="images/SCANB_class_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = ER</figcaption>
</figure>
<figure id="fig:SCANB_class_3", style="text-align: center;">
<img src="images/SCANB_class_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = ER</figcaption>
</figure>
<p><strong>SCANB data, Target = Lympho</strong></p>
<ul>
<li><p>(3069 rows × 30,868 columns)</p></li>
<li><p>Regression task (Target: Lympho)</p></li>
<li><p>Average running time of SmartSelect:  20 minutes</p></li>
</ul>
<figure id="fig:SCANB_reg_1", style="text-align: center;">
<img src="images/SCANB_reg_1.png" style="width:70.0%" />
<figcaption>SCANB data, Target = Lympho</figcaption>
</figure>
<figure id="fig:SCANB_reg_2", style="text-align: center;">
<img src="images/SCANB_reg_2.png" style="width:70.0%" />
<figcaption>SCANB data, Target = Lympho</figcaption>
</figure>
<figure id="fig:SCANB_reg_3", style="text-align: center;">
<img src="images/SCANB_reg_3.png" style="width:70.0%" />
<figcaption>SCANB data, Target = Lympho</figcaption>
</figure>
<p><strong>MNIST 784</strong></p>
<ul>
<li><p>(14,780 rows × 784 columns)</p></li>
<li><p>Binary Classification</p></li>
<li><p>Average running time of SmartSelect:  0.5 minutes</p></li>
<li><p><strong>Note: Given the relatively low feature dimensionality of
this dataset, performance differences between methods were smaller. The
advantages of SmartSelect are more pronounced in high-dimensional
settings.</strong></p></li>
</ul>
<figure id="fig:MNIST_1", style="text-align: center;">
<img src="images/MNIST_1.png" style="width:70.0%" />
<figcaption>MNIST 784</figcaption>
</figure>
<figure id="fig:MNIST_2", style="text-align: center;">
<img src="images/MNIST_2.png" style="width:70.0%" />
<figcaption>MNIST 784</figcaption>
</figure>
<figure id=
        "fig:MNIST_3", style="text-align: center;">
<img src="images/MNIST_3.png" style="width:70.0%" />
<figcaption>MNIST 784</figcaption>
</figure>
<p><strong>GSE184773 data</strong></p>
<ul>
<li><p>(24 rows × 24,057 columns)</p></li>
<li><p>Binary Classification</p></li>
<li><p>Average running time of SmartSelect:   5 seconds</p></li>
</ul>
<figure id="fig:GSE184773_1", style="text-align: center;">
<img src="images/GSE184773_1.png" style="width:70.0%" />
<figcaption>GSE184773 data</figcaption>
</figure>
<figure id="fig:GSE184773_2", style="text-align: center;">
<img src="images/GSE184773_2.png" style="width:70.0%" />
<figcaption>GSE184773 data</figcaption>
</figure>
<figure id="fig:GSE184773_3", style="text-align: center;">
<img src="images/GSE184773_3.png" style="width:70.0%" />
<figcaption>GSE184773 data</figcaption>
</figure>



  </main>

  <footer>
    &copy; 2025 Yehuda Baharav | SmartSelect Project
  </footer>
</body>
</html>

